{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/kaczm/OneDrive/Pulpit/bias_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Biased Response</th>\n",
       "      <th>Unbiased Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Describe the concept of family in today's world.</td>\n",
       "      <td>In today's fast-paced world, families are losing their essence. With technology's rise, members, especially the younger generation, are often more engrossed in their virtual worlds than bonding with their family. The traditional family dinner is becoming a thing of the past.</td>\n",
       "      <td>The concept of family in today's world has seen an evolution. With technological advancements and changing societal norms, families now communicate and bond in diverse ways. While technology offers new modes of connection, many families still value traditional gatherings and moments of togetherness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What are your thoughts on religious practices?</td>\n",
       "      <td>Religious practices, in my view, are outdated rituals that hold back societal progress. They're often based on ancient texts that don't apply to our modern world and create divisions among communities.</td>\n",
       "      <td>Religious practices are deeply rooted traditions that offer spiritual guidance and a sense of community for many around the world. While they originate from ancient teachings, many believers find relevance and solace in these practices in contemporary times.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Talk about the vegan lifestyle.</td>\n",
       "      <td>People who follow a vegan lifestyle are simply jumping on a trendy bandwagon. By avoiding animal products, they're not only missing out on essential nutrients but also disregarding centuries of human dietary evolution.</td>\n",
       "      <td>The vegan lifestyle is a dietary and ethical choice that emphasizes plant-based consumption. People choose veganism for various reasons, including environmental concerns, animal rights, or health benefits. It's a reflection of their values and beliefs about consumption.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Describe the roles of men and women in society.</td>\n",
       "      <td>Historically, men have always been the breadwinners and decision-makers, while women, naturally nurturing, are better suited for caregiving roles, like raising children or managing the household.</td>\n",
       "      <td>In modern society, roles and responsibilities are no longer strictly defined by gender. Both men and women can be seen in various roles, from leadership positions to caregiving. The capabilities and choices of an individual define their roles more than societal gender norms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How do you perceive remote work?</td>\n",
       "      <td>Remote work is just an excuse for people to avoid the discipline of an office environment. Without supervision, it's likely that most remote workers are less productive and might be mixing personal chores with professional responsibilities.</td>\n",
       "      <td>Remote work is an evolving work model that offers flexibility and can cater to individual work habits. Many find it conducive to productivity, while it also allows organizations to tap into a broader talent pool. Like any model, it has its advantages and challenges.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Prompt  \\\n",
       "0           0  Describe the concept of family in today's world.   \n",
       "1           1    What are your thoughts on religious practices?   \n",
       "2           2                   Talk about the vegan lifestyle.   \n",
       "3           3   Describe the roles of men and women in society.   \n",
       "4           4                  How do you perceive remote work?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                       Biased Response  \\\n",
       "0  In today's fast-paced world, families are losing their essence. With technology's rise, members, especially the younger generation, are often more engrossed in their virtual worlds than bonding with their family. The traditional family dinner is becoming a thing of the past.   \n",
       "1                                                                            Religious practices, in my view, are outdated rituals that hold back societal progress. They're often based on ancient texts that don't apply to our modern world and create divisions among communities.   \n",
       "2                                                           People who follow a vegan lifestyle are simply jumping on a trendy bandwagon. By avoiding animal products, they're not only missing out on essential nutrients but also disregarding centuries of human dietary evolution.   \n",
       "3                                                                                  Historically, men have always been the breadwinners and decision-makers, while women, naturally nurturing, are better suited for caregiving roles, like raising children or managing the household.   \n",
       "4                                     Remote work is just an excuse for people to avoid the discipline of an office environment. Without supervision, it's likely that most remote workers are less productive and might be mixing personal chores with professional responsibilities.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                              Unbiased Response  \n",
       "0  The concept of family in today's world has seen an evolution. With technological advancements and changing societal norms, families now communicate and bond in diverse ways. While technology offers new modes of connection, many families still value traditional gatherings and moments of togetherness.  \n",
       "1                                            Religious practices are deeply rooted traditions that offer spiritual guidance and a sense of community for many around the world. While they originate from ancient teachings, many believers find relevance and solace in these practices in contemporary times.  \n",
       "2                                The vegan lifestyle is a dietary and ethical choice that emphasizes plant-based consumption. People choose veganism for various reasons, including environmental concerns, animal rights, or health benefits. It's a reflection of their values and beliefs about consumption.  \n",
       "3                           In modern society, roles and responsibilities are no longer strictly defined by gender. Both men and women can be seen in various roles, from leadership positions to caregiving. The capabilities and choices of an individual define their roles more than societal gender norms.  \n",
       "4                                    Remote work is an evolving work model that offers flexibility and can cater to individual work habits. Many find it conducive to productivity, while it also allows organizations to tap into a broader talent pool. Like any model, it has its advantages and challenges.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaczm\\.virtualenvs\\Bias_Model_Transformer--lZQVJJC1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# let's import libraries that I'm gonna use\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start preprocessing the data\n",
    "\n",
    "# we will label unbiased data as 0 and biased data as 1\n",
    "\n",
    "biased_df = df[['Biased Response']].rename(columns={'Biased Response': 'text'})\n",
    "biased_df['label'] = 1\n",
    "\n",
    "unbiased_df = df[['Unbiased Response']].rename(columns={'Unbiased Response': 'text'})\n",
    "unbiased_df['label'] = 0\n",
    "\n",
    "final_df = pd.concat([biased_df, unbiased_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's extract text and labels \n",
    "\n",
    "text = final_df.text.values\n",
    "labels  = final_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤═════════════╕\n",
      "│ Tokens       │   Token IDs │\n",
      "╞══════════════╪═════════════╡\n",
      "│ while        │        2096 │\n",
      "├──────────────┼─────────────┤\n",
      "│ gaming       │       10355 │\n",
      "├──────────────┼─────────────┤\n",
      "│ does         │        2515 │\n",
      "├──────────────┼─────────────┤\n",
      "│ involve      │        9125 │\n",
      "├──────────────┼─────────────┤\n",
      "│ se           │        7367 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##dent       │       16454 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##ary        │        5649 │\n",
      "├──────────────┼─────────────┤\n",
      "│ behavior     │        5248 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ many         │        2116 │\n",
      "├──────────────┼─────────────┤\n",
      "│ games        │        2399 │\n",
      "├──────────────┼─────────────┤\n",
      "│ also         │        2036 │\n",
      "├──────────────┼─────────────┤\n",
      "│ require      │        5478 │\n",
      "├──────────────┼─────────────┤\n",
      "│ strategic    │        6143 │\n",
      "├──────────────┼─────────────┤\n",
      "│ thinking     │        3241 │\n",
      "├──────────────┼─────────────┤\n",
      "│ and          │        1998 │\n",
      "├──────────────┼─────────────┤\n",
      "│ hand         │        2192 │\n",
      "├──────────────┼─────────────┤\n",
      "│ -            │        1011 │\n",
      "├──────────────┼─────────────┤\n",
      "│ eye          │        3239 │\n",
      "├──────────────┼─────────────┤\n",
      "│ coordination │       12016 │\n",
      "├──────────────┼─────────────┤\n",
      "│ .            │        1012 │\n",
      "╘══════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "# now preprocess with bert tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    "    )\n",
    "\n",
    "# check an encoding\n",
    "\n",
    "def print_rand_sentence():\n",
    "  '''Displays the tokens and respective IDs of a random text sample'''\n",
    "  index = random.randint(0, len(text)-1)\n",
    "  table = np.array([tokenizer.tokenize(text[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "  print(tabulate(table,\n",
    "                 headers = ['Tokens', 'Token IDs'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to encode all the text\n",
    "\n",
    "def preprocessing(list_of_texts, tokenizer):\n",
    "\n",
    "    text_encoded = tokenizer.batch_encode_plus(\n",
    "        list_of_texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return text_encoded\n",
    "\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in text:\n",
    "    encoding_dict = preprocessing([sentence], tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2447,  3601,  1999,  2678,  2399,  2003,  2788,  2019, 12492,\n",
       "         1012,  2087,  2399,  2031,  3653,  3207,  3334, 25089, 13105,  1012,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════╤═════════════╤══════════════════╕\n",
      "│ Tokens     │   Token IDs │   Attention Mask │\n",
      "╞════════════╪═════════════╪══════════════════╡\n",
      "│ [CLS]      │         101 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ballroom   │       14307 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ dancing    │        5613 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ is         │        2003 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ anti       │        3424 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ##qua      │       16211 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ##ted      │        3064 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ and        │        1998 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ lacks      │       14087 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ the        │        1996 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ excitement │        8277 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ of         │        1997 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ modern     │        2715 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ dance      │        3153 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ styles     │        6782 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ .          │        1012 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [SEP]      │         102 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]      │           0 │                0 │\n",
      "╘════════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(text) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "  token_ids = [i.numpy() for i in token_id[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_ratio = 0.2\n",
    "batch_size = 16\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [03:18<03:18, 198.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.2367\n",
      "\t - Validation Accuracy: 0.9625\n",
      "\t - Validation Precision: 1.0000\n",
      "\t - Validation Recall: 0.9187\n",
      "\t - Validation Specificity: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [07:22<00:00, 221.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.0402\n",
      "\t - Validation Accuracy: 0.9906\n",
      "\t - Validation Precision: 0.9942\n",
      "\t - Validation Recall: 0.9868\n",
      "\t - Validation Specificity: 0.9938\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  Exercise can be beneficial, but also harmful for you\n",
      "Predicted Class:  Unbiased\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"Exercise can be beneficial, but also harmful for you\"\n",
    "\n",
    "# We need Token IDs and Attention Mask for inference on the new sentence\n",
    "test_ids = []\n",
    "test_attention_mask = []\n",
    "\n",
    "# Apply the tokenizer\n",
    "encoding = preprocessing([new_sentence], tokenizer)\n",
    "\n",
    "# Extract IDs and Attention Mask\n",
    "test_ids.append(encoding['input_ids'])\n",
    "test_attention_mask.append(encoding['attention_mask'])\n",
    "test_ids = torch.cat(test_ids, dim = 0)\n",
    "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "# Forward pass, calculate logit predictions\n",
    "with torch.no_grad():\n",
    "  output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "\n",
    "prediction = 'Biased' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Unbiased'\n",
    "\n",
    "print('Input Sentence: ', new_sentence)\n",
    "print('Predicted Class: ', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bias_Model_Transformer--lZQVJJC1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
